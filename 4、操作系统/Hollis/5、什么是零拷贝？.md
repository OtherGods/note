#BufferedI/O #PageCache #DerictI/O #零拷贝 #上下文切换 #DMA #CPU不参与的数据拷贝  #CPU参与的数据拷贝 #mmap #sendfile 

关于`Buffered I/O（缓冲I/O）`读写数据：[write和fsync的区别是什么？](4、操作系统/Hollis/write和fsync的区别是什么？.md)

# 1、典型回答

要想理解零拷贝，首先要了解操作系统的IO流程（[buffered I/O（缓冲 I/O）](5、什么是零拷贝？#2.5、buffered%20I/O（缓冲%20I/O）)），因为有内核态和用户态的区别，为了保证安全性和缓存，普通的读写流程如下：

（对于Java程序，还会多了一个堆外内存和堆内存之间的copy）
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201623766.jpg)

`read+write`整体的流程如下所示：
1. 用户read发起系统调用，由用户态进入内核态，通过DMA技术<font color="red" size=5>将磁盘中的数据copy到内核缓冲区中</font>
2. 当DMA完成工作后，会发起一个中断通知CPU数据拷贝完成，然后CPU再<font color="red" size=5>将内核态中的数据copy到用户态中</font>
3. 内核唤醒对应线程，同时将用户态的数据返回给该线程空间
4. 用户态线程进行业务处理
5. 当服务器对请求进行响应的时候，会发起系统调用，由内核<font color="red" size=5>将用户态的数据copy到内核态中</font>
6. 复制完毕后，再有网络适配器通过DMA技术<font color="red" size=5>将内核态缓冲区中的数据copy到网卡中</font>，完成后，内核态会返回到用户态
7. 最后由网卡将数据发送出去

> DMA：**Direct Memory Access，直接内存访问**， 是一种计算机硬件技术，允许**外部设备绕过 CPU 直接访问系统内存**，用于高效传输数据。

在这个过程中，如果不考虑用户态的内存拷贝和物理设备到驱动的数据拷贝，我们会发现，这其中会涉及4次数据拷贝。同时也会涉及到4次进程上下文的切换。

**==上下文切换四次的原因：（我分析的）==**
1. 调用`read`读取数据时用户态切换到内核态
2. `read`的返回：数据从物理设备读取到用户态后，还要从内核态切换回用户态（这样才能再调用`write`）
3. 调用`write`将修改后的数据写到输出设备
4. `write`的返回：从内核态切换回用户态（这样用户态的线程才能继续执行）

所谓的<font color="red" size=5>零拷贝</font>，作用就是<font color="red" size=5>通过各种方式，减少数据拷贝的次数/减少CPU参与数据拷贝的次数</font>。

传统I/O流程存在的问题：
- **CPU 被绑架**：每次传输需消耗数千时钟周期（取指、解码、执行）。
- **频繁中断**：每传输少量数据就触发中断，上下文切换开销大。
- **内存带宽浪费**：数据需经过CPU寄存器中转（而非直通内存）。

常见的零拷贝方式有mmap，sendfile，dma，directI/O等。

# 2、扩展知识
## 2.1、DMA

<font color="red" size=5>正常的IO流程中，不管是物理设备之间的数据拷贝，如：磁盘到内存，还是内存之间的数据拷贝，如：用户态到内核态，<font color="blue" size=5>都是需要CPU参与</font>的，如下所示</font>

总线内存CPU硬件
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201634674.jpg)

如果是比较大的文件，这样无意义的copy显然会极大的浪费CPU的效率，所以就诞生了DMA

DMA的全称是Direct Memory Access，顾名思义，<font color="red" size=5>DMA的作用就是直接将IO设备的数据拷贝到内核缓冲区中</font>。使用DMA的<font color="blue" size=5>好处就是IO设备到内核之间的数据拷贝不需要CPU的参与</font>，CPU只需要给DMA发送copy指令即可，提高了处理器的利用效率，如下所示：
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201635997.jpg)

## 2.2、mmap

上文我们说到，正常的 **==read+write==**，都会经历至少四次数据拷贝的，其中就包括内核态到用户态的拷贝，它的作用是为了安全和缓存。如果我们能保证安全性，是否就让用户态和内核态共享一个缓冲区呢？这就是mmap的作用。

mmap，全称是`memory map`，翻译过来就是**内存映射**，顾名思义，就是 **==将内核态和用户态的内存映射到一起，避免来回拷贝==**，实现这样的映射关系后，进程就可以采用指针的方式读写操作这一段内存，而系统会自动回写脏页面到对应的文件磁盘上，即完成了对文件的操作而不必再调用 read、write 等系统调用函数。相反，内核空间对这段区域的修改也直接反映用户空间，从而可以实现不同进程间的文件共享。其函数签名如下：
```Basic
void *mmap(void *addr, size_t length, int prot, int flags, int fd, off_t offset);
```

一般来讲，**==mmap会代替read方法==**，模型如下图所示：
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201638706.jpg)

如果这个时候系统进行IO的话，采用mmap + write的方式，内存拷贝的次数会变为3次，上下文切换则依旧是4次。

**==上下文切换四次的原因：（我分析的）==**
1. 调用`mmap`读取数据时用户态切换到内核态
2. `mmap`的返回：数据从物理设备读取到内核态后，还要从内核态切换回用户态（这样才能再调用`write`）
3. 调用`write`将修改后的数据写到输出设备
4. `write`的返回：从内核态切换回用户态（这样用户态的线程才能继续执行）

需要注意的是，mmap采用基于缺页异常的懒加载模式。通过 mmap 申请 1000G 内存可能仅仅占用了 100MB 的虚拟内存空间，甚至没有分配实际的物理内存空间，只有当真正访问的时候，才会通过缺页中断的方式分配内存

**但是mmap不是银弹**，有如下原因：
1. mmap 使用时必须实现指定好内存映射的大小，因此 mmap 并不适合变长文件；
2. 因为mmap在文件更新后会通过OS自动将脏页回写到disk中，所以在随机写很多的情况下，mmap 方式在效率上不一定会比带缓冲区的一般写快；
3. 因为mmap必须要在内存中找到一块连续的地址块，如果在 32-bits 的操作系统上，虚拟内存总大小也就 2GB左右（32位系统的地址空间最大为4G，除去1G系统，用户能使用的内存最多为3G左右（windows内核较大，一般用户只剩下2G可用）。），此时就很难对 4GB 大小的文件完全进行 mmap，所以对于超大文件来讲，mmap并不适合

## 2.3、sendfile

如果 **==只是传输数据，并不对数据作任何处理==**，譬如将服务器存储的静态文件，如html，js发送到客户端用于浏览器渲染，在这种场景下，如果依然进行这么多数据拷贝和上下文切换，简直就是丧心病狂有木有！所以我们就可以通过sendfile的方式，只做文件传输，而不通过用户态进行干预：
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201641976.jpg)

此时我们发现，数据拷贝变成了3次，上下文切换减少到了2次。

**==上下文切换两次的原因：（我分析的）==**
1. 调用`sendfile`传输数据时用户态切换到内核态
2. `sendfile`的返回：发送完数据后，还要从内核态切换回用户态（这样才能再调用`write`）

虽然这个时候已经优化了不少，但是我们还有一个问题，为什么内核要拷贝两次（page cache -> socket cache），能不能省略这个步骤？当然可以

### 2.3.1、sendfile + DMA Scatter/Gather

DMA gather是LInux2.4新引入的功能，它可以读page cache中的数据描述信息（内存地址和偏移量）记录到socket cache中，由 DMA 根据这些将数据从读缓冲区拷贝到网卡，相**比之前版本减少了一次CPU拷贝的过程**，如下图所示：
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201642904.jpg)

## 2.4、direct I/O

*==磁盘 I/O 操作方式==*

**==核心思想：绕过操作系统的页缓存，让应用程序直接与磁盘进行数据交换==**
**工作流程：**
1. **写操作（Write）**：应用程序发起写请求时，数据**直接**从应用程序的内存空间写入磁盘。操作系统不会在页缓存中保留副本。写操作必须等待数据真正落到磁盘（或磁盘的带电池缓存中）才会返回。
2. **读操作（Read）**：应用程序发起读请求时，数据**直接**从磁盘读取到应用程序的内存空间，不会填充操作系统的页缓存。

之前的mmap可以让用户态和内核态共用一个内存空间来减少拷贝，其实还有一个方式，就是 *==硬件数据不经过内核态的空间，直接到用户态的内存中==*，这种方式就是Direct I/O。换句话说，**==Direct I/O不会经过内核态，而是用户态和设备的直接交互，用户态的写入就是直接写入到磁盘，不会再经过操作系统刷盘处理==**。

这样确实拷贝次数减少，读取速度会变快，但是因为操作系统不再负责缓存之类的管理，这就必须交由应用程序自己去做，譬如MySql就是自己通过Direct I/O完成的，同时MySql也有一套自己的缓存系统：
> *==MySQL中使用 `Direct I/O` 来读写数据文件（`.ibd`）==*，以避免其 `Buffer Pool` 和 OS `Page Cache` 之间的双缓冲。
> 
> 但同时，*==MySQL通常使用 `Buffered I/O` 来写重做日志（Redo Log）==*，因为日志是顺序追加写入，利用 Page Cache 的异步刷盘机制可以获得极高的吞吐量（空间局部性原理），然后定期调用 `fsync()` 来保证持久性。这完美展示了如何根据不同的需求混合使用两种 I/O 模式。

同时，虽然direct I/O可以直接将文件写入磁盘中，但是文件相关的元信息还是要通过fsync缓存到内核空间中

## 2.5、buffered I/O（缓冲 I/O）

*==磁盘 I/O 操作方式==*

**==核心思想：利用操作系统的 页缓存（Page Cache） 作为应用程序和磁盘之间的中间层，以提升性能。==**
工作流程：
1. **写操作（Write）**：当应用程序发起写请求时，数据并不会立即被写入磁盘。而是先被复制到操作系统内核的**页缓存（Page Cache）** 中。此时，写操作就成功返回了。操作系统会在后台（由 `pdflush` 内核线程等）异步地将页缓存中的数据刷新（flush）到物理磁盘上。
2. **读操作（Read）**：当应用程序发起读请求时，操作系统首先检查请求的数据是否已经在页缓存中。如果在（称为“缓存命中”），则直接从内存中返回数据，速度极快。如果不在（称为“缓存未命中”），则从磁盘读取数据到页缓存，然后再拷贝到应用程序的内存空间。

[典型回答](4、操作系统/Hollis/5、什么是零拷贝？.md#1、典型回答) 中Read操作就用到了页缓存：
![1.jpg](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202404201623766.jpg)

### 2.5.1、Direct I/O 与 Buffered I/O对比

![RocketMQ最佳实践.png](https://raw.githubusercontent.com/OtherGods/MaterialImage/main/img/202508311702808.png)

1. **Buffered I/O（缓冲 I/O）**：<font color="blue" size=5>数据必须经过内核的 Page Cache 中转</font>。这*使得读写操作更快返回，提升性能，但也引入了数据延迟落盘的风险*。
   - **提升性能**：极大地减少了直接访问磁盘的次数
   - **智能预读（Read-ahead）**：操作系统会预测应用程序接下来可能需要的数据，并提前将其加载到页缓存中，从而优化顺序读取的性能。
   - **统一缓存**：所有进程共享页缓存。如果多个进程访问同一个文件，它们可以受益于同一份缓存数据。
2. **Direct I/O（直接 I/O）**：<font color="blue" size=5>数据直达磁盘</font>。这使得应用程序对数据何时落地有完全的控制权，保证了更强的一致性，但*代价是每次操作都必须等待物理磁盘完成，增加了延迟*。
   - **数据一致性更强**：写操作返回成功即意味着数据已持久化（在硬件保证范围内），减少了因系统崩溃导致的数据丢失窗口。
   - **避免双缓冲**：对于有自己缓存管理的应用程序（如数据库），可以避免内存的重复使用，让应用程序更高效地管理所有可用内存。
   - **可预测的 I/O**：应用程序对数据何时落盘有完全的控制权，更适合对延迟有严格要求的场景。
   - **减少 CPU 开销**：省去了在用户态和内核态之间来回拷贝数据的过程。

|特性|Buffered I/O|Direct I/O|
|---|---|---|
|**核心机制**|使用 OS **页缓存（Page Cache）** 作为中间层|**绕过** OS 页缓存，直通磁盘|
|**数据一致性**|较弱，存在数据丢失风险（需 `fsync`）|较强，写成功通常意味着已持久化|
|**性能特点**|**读性能优**（缓存命中时），**写性能优**（异步）|**读性能依赖应用缓存**，**写延迟稳定**|
|**内存使用**|可能造成**双缓冲**，浪费内存|高效，**避免双缓冲**，内存全由应用管理|
|**控制权**|由 OS 控制缓存和刷盘策略|由**应用程序**完全控制 I/O 行为|
|**适用场景**|通用应用、文本文件、日志写入|**数据库**、虚拟机磁盘、大型文件传输、自定义缓存的应用|
|**CPU 开销**|较高（需要在内核和用户空间之间拷贝数据）|较低（减少数据拷贝次数）|

### 2.5.2、为什么需要使用 Direct I/O

既然 Buffered I/O 性能更好，为什么还需要 Direct I/O？主要为了解决以下问题：
1. **避免双重缓存（Double Buffering）**  
    许多高级应用（如数据库：MySQL, Oracle）已经有自己高度优化、复杂的缓存机制（如 InnoDB的Buffer Pool）。如果使用 Buffered I/O，数据会被缓存两次：一次在应用的缓存，一次在 OS 的 Page Cache。这是一种**内存浪费**。使用 Direct I/O 可以让应用独占缓存管理，更高效地利用内存。
2. **数据一致性控制（Control over Data Durability）**  
    Buffered I/O 的写操作在数据进入 Page Cache 后就返回成功，而非真正写入磁盘。如果此时系统崩溃，数据会丢失。虽然可以调用 `fsync()` 强制刷盘，但这增加了复杂性。  
    Direct I/O 的写操作会等待数据真正到达磁盘（或磁盘的带电池写缓存）后才返回。这让应用程序能**精确控制**数据落盘的时机，保证事务的持久性。
3. **可预测的性能（Predictable Performance）**  
    Buffered I/O 的刷盘时机由操作系统内核决定，这可能在某些时候引发不可预测的 I/O 尖峰（例如，当 Page Cache 需要清理大量脏页时）。Direct I/O 让应用程序自己决定何时进行 I/O，使得 I/O 负载更加可控和均匀。
